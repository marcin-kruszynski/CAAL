# CAAL Voice Framework Configuration
# Copy to .env and update values

# =============================================================================
# Platform - Choose your hardware
# =============================================================================
#
# NVIDIA GPU (Linux):
#   - Run: docker compose up -d
#   - Uses Wyoming protocol STT/TTS (external services)
#
# Apple Silicon (M1/M2/M3/M4):
#   - Run: docker compose -f docker-compose.apple.yaml up -d
#   - Uses mlx-audio on host for STT/TTS (Metal acceleration)
#   - See "Apple Silicon Setup" section below
#
# =============================================================================
# Network Mode - Choose ONE option below
# =============================================================================
#
# OPTION A: LAN HTTP (default)
#   - Access via http://<LAN_IP>:3000
#   - Run: docker compose up -d
#   - Voice works from host machine, text-only from other devices
#
# OPTION B: LAN HTTPS (mkcert)
#   - Access via https://<LAN_IP>
#   - Run: docker compose --profile https up -d
#   - Full voice from any device on LAN
#   - Requires: mkcert for local certs (see README)
#
# OPTION C: Tailscale HTTPS (remote access)
#   - Access via https://<TAILSCALE_DOMAIN>
#   - Run: docker compose --profile https up -d
#   - Full voice from anywhere
#   - Requires: tailscale cert <domain> (see README)
#
# -----------------------------------------------------------------------------

# OPTION A: LAN HTTP (default)
# Find your LAN IP: ip addr show | grep "inet " | grep -v 127.0.0.1
CAAL_HOST_IP=192.168.1.100

# OPTION B: LAN HTTPS with mkcert (uncomment HTTPS_DOMAIN, keep CAAL_HOST_IP)
# HTTPS_DOMAIN=192.168.1.100

# OPTION C: Tailscale HTTPS (uncomment both, update CAAL_HOST_IP to Tailscale IP)
# CAAL_HOST_IP=100.x.x.x                         # tailscale ip -4
# HTTPS_DOMAIN=your-machine.tailnet.ts.net       # tailscale cert <this-value>

# =============================================================================
# LiveKit Configuration
# =============================================================================
LIVEKIT_URL=ws://localhost:7880
LIVEKIT_API_KEY=devkey
LIVEKIT_API_SECRET=secret

# =============================================================================
# STT Configuration (Wyoming Protocol)
# =============================================================================
# Wyoming STT service URI (faster-whisper-wyoming)
# Format: tcp://host:port
WYOMING_STT_URL=tcp://nabu.home:10300

# =============================================================================
# TTS Configuration (Wyoming Protocol)
# =============================================================================
# Wyoming TTS service URI (piper-wyoming)
# Format: tcp://host:port
WYOMING_TTS_URL=tcp://nabu.home:10200

# TTS voice name (depends on your Wyoming TTS service configuration)
TTS_VOICE=

# =============================================================================
# LLM (llama.cpp server)
# =============================================================================
# llama.cpp server URL (OpenAI-compatible API endpoint)
# Use your machine's IP, not localhost
LLAMACPP_HOST=http://llama.home/v1

# Model name (configured on your llama.cpp server)
LLAMACPP_MODEL=gpt-oss-20b-mxfp4

# Temperature for LLM responses (0.0-1.0, lower = more deterministic)
LLAMACPP_TEMPERATURE=0.7

# Context window size (default 8192, increase for better tool response handling)
LLAMACPP_NUM_CTX=8192

# Max conversation turns to keep in sliding window (prevents context overflow)
LLAMACPP_MAX_TURNS=20

# Number of tool responses to cache for follow-up queries
TOOL_CACHE_SIZE=3

# =============================================================================
# MCP Server Configuration
# =============================================================================
# n8n is the foundational MCP server, configured here in .env
# Additional MCP servers can be configured in mcp_servers.json (see mcp_servers.json.example)

# n8n MCP endpoint URL (required for workflow tools)
N8N_MCP_URL=https://n8n.home/mcp-server/http

# n8n MCP access token (required - get from n8n Settings > MCP Access)
N8N_MCP_TOKEN=your_n8n_mcp_token_here

# =============================================================================
# Wake Word Detection (Picovoice Porcupine)
# =============================================================================
# Get your access key from https://console.picovoice.ai/
# Leave empty to disable wake word feature
# Also requires hey_cal.ppn and porcupine_params.pv in frontend/public/
PORCUPINE_ACCESS_KEY=

# =============================================================================
# General
# =============================================================================
# IANA timezone ID - see: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
TIMEZONE=America/Los_Angeles
# Display name spoken by CAAL (optional, defaults to "Pacific Time")
TIMEZONE_DISPLAY=Pacific Time

# =============================================================================
# Apple Silicon Setup (M1/M2/M3/M4)
# =============================================================================
# Docker can't access Metal GPU, so STT/TTS run on host via mlx-audio.
# See README.md for full setup instructions.
#
# 1. Install: pip install "mlx-audio[all]"
# 2. Start server: python -m mlx_audio.server --host 0.0.0.0 --port 8000
# 3. Pre-load models:
#    curl -X POST "http://localhost:8000/v1/models?model_name=mlx-community/whisper-medium-mlx"
#    curl -X POST "http://localhost:8000/v1/models?model_name=prince-canuma/Kokoro-82M"
# 4. Run: docker compose -f docker-compose.apple.yaml up -d
#
# mlx-audio URL (defaults to host.docker.internal:8000 for local Mac)
# MLX_AUDIO_URL=http://host.docker.internal:8000
#
# Optional: Override default MLX models
# WHISPER_MODEL=mlx-community/whisper-medium-mlx
# TTS_MODEL=prince-canuma/Kokoro-82M
#
# Other whisper options: whisper-tiny, whisper-small, whisper-large-v3-turbo
